---
title: "Homework 3"
author: "Jingyuan Wu"
date: "2/9/2022"
output: github_document
---

# Instruction
Using the RMarkdown/knitr/github mechanism, implement the following tasks:

Use the prostate cancer data.

Use the cor function to reproduce the correlations listed in HTF Table 3.1, page 50.

Treat lcavol as the outcome, and use all other variables in the data set as predictors.

With the training subset of the prostate data, train a least-squares regression model with all predictors using the lm function.

Use the testing subset to compute the test error (average squared-error loss) using the fitted least-squares regression model.

Train a ridge regression model using the glmnet function, and tune the value of lambda (i.e., use guess and check to find the value of lambda that approximately minimizes the test error).

Create a figure that shows the training and test error associated with ridge regression as a function of lambda

Create a path diagram of the ridge regression analysis, similar to HTF Figure 3.8

```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(glmnet)
prostate <- 
  read.table(url(
    'https://web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data'))
```

```{r}
## generate the correlations
cor(prostate)
```

```{r}
prostate_train <- prostate %>%
  filter(train == TRUE) %>% 
  select(-train)
prostate_test <- prostate %>%
  filter(train == FALSE) %>% 
  select(-train)

## train a least-squares regression model with all predictors
fit <- lm(lcavol ~ ., data=prostate_train)

## L2 loss function
L2_loss <- function(y, yhat)
  (y-yhat)^2

## function to compute testing/training error with least-squares regression model
error <- function(dat, fit, loss=L2_loss)
  mean(loss(dat$lcavol, predict(fit, newdata=dat)))

## compute test error (average squared-error loss) using the fitted least-squares regression model
error(prostate_test, fit)
```

```{r}
## train a ridge regression
form  <- lcavol ~  lweight + age + lbph + lcp + pgg45 + lpsa + svi + gleason
x_inp <- model.matrix(form, data=prostate_train)
y_out <- prostate_train$lcavol

fit <- glmnet(x=x_inp, y=y_out, alpha=0, lambda=seq(0.5, 0, -0.05))
#print(fit$beta)

## functions to compute testing/training error with glmnet
error <- function(dat, fit, lam, form, loss=L2_loss) {
  x_inp <- model.matrix(form, data=dat)
  y_out <- dat$lcavol
  y_hat <- predict(fit, newx=x_inp, s=lam)  ## see predict.elnet
  mean(loss(y_out, y_hat))
}

## function to find lambda with least test error
find_lambda <- function(dat) {
  test_error_min=1000000
  for (i in dat){
    test_error=error(prostate_test, fit, lam=i, form=form)
    if (test_error_min>test_error){
      test_error_min=test_error
      l=i
    }
  }
  print(l)
}

find_lambda(seq(0.5, 0, -0.01))
```

We can find the value of lambda that approximately minimizes the test error is about 0.12.

```{r}
## testing error at lambda=0.12
error(prostate_test, fit, lam=0.12, form=form)

## compute training and testing errors as function of lambda
err_train_1 <- sapply(fit$lambda, function(lam) 
  error(prostate_train, fit, lam, form))
err_test_1 <- sapply(fit$lambda, function(lam) 
  error(prostate_test, fit, lam, form))

## plot the training and test error associated with the ridge regression as a function of lambda
plot(x=range(fit$lambda),
     y=range(c(err_train_1, err_test_1)),
     xlim=rev(range(fit$lambda)),
     type='n',
     xlab=expression(lambda),
     ylab='train/test error')
points(fit$lambda, err_train_1, pch=19, type='b', col='darkblue')
points(fit$lambda, err_test_1, pch=19, type='b', col='darkred')
legend('bottomleft', c('train','test'), lty=1, pch=19,
       col=c('darkblue','darkred'), bty='n')

#colnames(fit$beta) <- paste('lam =', fit$lambda)
#print(fit$beta %>% as.matrix)

## plot path diagram of the ridge regression analysis
plot(x=range(fit$lambda),
     y=range(as.matrix(fit$beta)),
     type='n',
     xlab=expression(lambda),
     ylab='Coefficients')
for(i in 1:nrow(fit$beta)) {
  points(x=fit$lambda, y=fit$beta[i,], pch=19, col='#00000055')
  lines(x=fit$lambda, y=fit$beta[i,], col='#00000055')
}
text(x=0, y=fit$beta[,ncol(fit$beta)], 
     labels=rownames(fit$beta),
     xpd=NA, pos=4, srt=45)
abline(h=0, lty=3, lwd=2)
```
